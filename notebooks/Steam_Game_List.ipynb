{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import datetime as dt\n",
    "import statistics\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting game IDs from SteamSpy API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_request(pages):\n",
    "    \"\"\"\n",
    "    Return json-formatted response of a get request using number of pages provided\n",
    "\n",
    "    Parameters\n",
    "    --------\n",
    "    pages: integer (number of pages from SteamSpy you want to get)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    steamspy_data\n",
    "        json-formatted response (dict-like)\n",
    "    \"\"\"\n",
    "    steamspy_data_json = {}  # Dictionary to store the data\n",
    "\n",
    "    for page in range(pages+1): #added 1 to get data for all selected pages\n",
    "        url = f'https://steamspy.com/api.php?request=all&page={page}'\n",
    "        response = None\n",
    "        while response is None:\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 200:\n",
    "                    steamspy_data_json.update(response.json())\n",
    "                    print('Downloading page={} on {}'.format(page, time.asctime()))\n",
    "                else:\n",
    "                    print(f'Request for page {page} failed with status code: {response.status_code}')\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Request for page {page} failed with error: {e}\")\n",
    "            print('Sleeping for 70 seconds on {}'.format(time.asctime()))\n",
    "            time.sleep(70)  # Delay for 70 seconds between requests\n",
    "    return steamspy_data_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading page=0 on Wed Jun 14 01:02:40 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:02:40 2023\n",
      "Downloading page=1 on Wed Jun 14 01:03:51 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:03:51 2023\n",
      "Downloading page=2 on Wed Jun 14 01:05:01 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:05:01 2023\n",
      "Downloading page=3 on Wed Jun 14 01:06:12 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:06:12 2023\n",
      "Downloading page=4 on Wed Jun 14 01:07:22 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:07:22 2023\n",
      "Downloading page=5 on Wed Jun 14 01:08:33 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:08:33 2023\n",
      "Downloading page=6 on Wed Jun 14 01:09:44 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:09:44 2023\n",
      "Downloading page=7 on Wed Jun 14 01:10:54 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:10:54 2023\n",
      "Downloading page=8 on Wed Jun 14 01:12:05 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:12:05 2023\n",
      "Downloading page=9 on Wed Jun 14 01:13:15 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:13:15 2023\n",
      "Downloading page=10 on Wed Jun 14 01:14:26 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:14:26 2023\n",
      "Downloading page=11 on Wed Jun 14 01:15:37 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:15:37 2023\n",
      "Downloading page=12 on Wed Jun 14 01:16:47 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:16:47 2023\n",
      "Downloading page=13 on Wed Jun 14 01:17:58 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:17:58 2023\n",
      "Downloading page=14 on Wed Jun 14 01:19:08 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:19:08 2023\n",
      "Downloading page=15 on Wed Jun 14 01:20:19 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:20:19 2023\n",
      "Downloading page=16 on Wed Jun 14 01:21:30 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:21:30 2023\n",
      "Downloading page=17 on Wed Jun 14 01:22:40 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:22:40 2023\n",
      "Downloading page=18 on Wed Jun 14 01:23:51 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:23:51 2023\n",
      "Downloading page=19 on Wed Jun 14 01:25:01 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:25:01 2023\n",
      "Downloading page=20 on Wed Jun 14 01:26:12 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:26:12 2023\n",
      "Downloading page=21 on Wed Jun 14 01:27:22 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:27:22 2023\n",
      "Downloading page=22 on Wed Jun 14 01:28:33 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:28:33 2023\n",
      "Downloading page=23 on Wed Jun 14 01:29:44 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:29:44 2023\n",
      "Downloading page=24 on Wed Jun 14 01:30:54 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:30:54 2023\n",
      "Downloading page=25 on Wed Jun 14 01:32:05 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:32:05 2023\n",
      "Downloading page=26 on Wed Jun 14 01:33:15 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:33:15 2023\n",
      "Downloading page=27 on Wed Jun 14 01:34:26 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:34:26 2023\n",
      "Downloading page=28 on Wed Jun 14 01:35:37 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:35:37 2023\n",
      "Downloading page=29 on Wed Jun 14 01:36:47 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:36:47 2023\n",
      "Downloading page=30 on Wed Jun 14 01:37:58 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:37:58 2023\n",
      "Downloading page=31 on Wed Jun 14 01:39:08 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:39:08 2023\n",
      "Downloading page=32 on Wed Jun 14 01:40:19 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:40:19 2023\n",
      "Downloading page=33 on Wed Jun 14 01:41:30 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:41:30 2023\n",
      "Downloading page=34 on Wed Jun 14 01:42:40 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:42:40 2023\n",
      "Downloading page=35 on Wed Jun 14 01:43:51 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:43:51 2023\n",
      "Downloading page=36 on Wed Jun 14 01:45:01 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:45:01 2023\n",
      "Downloading page=37 on Wed Jun 14 01:46:12 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:46:12 2023\n",
      "Downloading page=38 on Wed Jun 14 01:47:23 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:47:23 2023\n",
      "Downloading page=39 on Wed Jun 14 01:48:33 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:48:33 2023\n",
      "Downloading page=40 on Wed Jun 14 01:49:44 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:49:44 2023\n",
      "Downloading page=41 on Wed Jun 14 01:50:54 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:50:54 2023\n",
      "Downloading page=42 on Wed Jun 14 01:52:05 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:52:05 2023\n",
      "Downloading page=43 on Wed Jun 14 01:53:16 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:53:16 2023\n",
      "Downloading page=44 on Wed Jun 14 01:54:26 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:54:26 2023\n",
      "Downloading page=45 on Wed Jun 14 01:55:37 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:55:37 2023\n",
      "Downloading page=46 on Wed Jun 14 01:56:47 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:56:47 2023\n",
      "Downloading page=47 on Wed Jun 14 01:57:58 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:57:58 2023\n",
      "Downloading page=48 on Wed Jun 14 01:59:08 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 01:59:08 2023\n",
      "Downloading page=49 on Wed Jun 14 02:00:19 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 02:00:19 2023\n",
      "Downloading page=50 on Wed Jun 14 02:01:30 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 02:01:30 2023\n",
      "Downloading page=51 on Wed Jun 14 02:02:40 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 02:02:40 2023\n",
      "Downloading page=52 on Wed Jun 14 02:03:51 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 02:03:51 2023\n",
      "Downloading page=53 on Wed Jun 14 02:05:01 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 02:05:01 2023\n",
      "Downloading page=54 on Wed Jun 14 02:06:12 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 02:06:12 2023\n",
      "Downloading page=55 on Wed Jun 14 02:07:23 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 02:07:23 2023\n",
      "Downloading page=56 on Wed Jun 14 02:08:33 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 02:08:33 2023\n",
      "Downloading page=57 on Wed Jun 14 02:09:44 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 02:09:44 2023\n",
      "Downloading page=58 on Wed Jun 14 02:10:54 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 02:10:54 2023\n",
      "Downloading page=59 on Wed Jun 14 02:12:05 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 02:12:05 2023\n",
      "Downloading page=60 on Wed Jun 14 02:13:16 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 02:13:16 2023\n",
      "Downloading page=61 on Wed Jun 14 02:14:26 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 02:14:26 2023\n",
      "Downloading page=62 on Wed Jun 14 02:15:37 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 02:15:37 2023\n",
      "Downloading page=63 on Wed Jun 14 02:16:47 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 02:16:47 2023\n",
      "Downloading page=64 on Wed Jun 14 02:17:58 2023\n",
      "Sleeping for 70 seconds on Wed Jun 14 02:17:58 2023\n"
     ]
    }
   ],
   "source": [
    "pages = 64  # Number of pages\n",
    "result_steamspy = get_request(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>appid</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>Team Fortress Classic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>Day of Defeat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>Deathmatch Classic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>Half-Life: Opposing Force</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   appid                       name\n",
       "0     10             Counter-Strike\n",
       "1     20      Team Fortress Classic\n",
       "2     30              Day of Defeat\n",
       "3     40         Deathmatch Classic\n",
       "4     50  Half-Life: Opposing Force"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save results from get_request as json file\n",
    "save_file = open('steamspy_all.json', 'w')\n",
    "json.dump(result_steamspy, save_file, indent=6)\n",
    "save_file.close()\n",
    "\n",
    "#  parse SteamSpy data into dataframe\n",
    "steamspy_all_appid = pd.DataFrame.from_dict(result_steamspy, orient='index')\n",
    "\n",
    "# export steam_spy_all to csv\n",
    "steamspy_all_appid.to_csv('/Users/sstefanovic/Documents/GitHub/LHL-Capstone_Project/data/steamspy_all_appid_raw.csv', index=False)\n",
    "\n",
    "# generate sorted app_list from SteamSpy data\n",
    "app_list = steamspy_all_appid[['appid', 'name']].sort_values('appid').reset_index(drop=True)\n",
    "\n",
    "# export app_list to csv\n",
    "app_list.to_csv('/Users/sstefanovic/Documents/GitHub/LHL-Capstone_Project/data/app_list.csv', index=False)\n",
    "\n",
    "# instead read from stored csv\n",
    "#app_list = pd.read_csv('../data/download/app_list.csv')\n",
    "\n",
    "# display first few rows\n",
    "app_list.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step - Getting all the data from SteamSpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting numpy array from the app_list dataframe\n",
    "list_of_appids = app_list.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "used_appid = []  # List to track used appids\n",
    "\n",
    "steamspy_columns = [\n",
    "    'appid', 'name', 'developer', 'publisher', 'score_rank', 'positive',\n",
    "    'negative', 'userscore', 'owners', 'average_forever', 'average_2weeks',\n",
    "    'median_forever', 'median_2weeks', 'price', 'initialprice', 'discount',\n",
    "    'languages', 'genre', 'ccu', 'tags'\n",
    "]\n",
    "\n",
    "\n",
    "def check_appid(app_id):\n",
    "    \"\"\"\n",
    "    Check if the given app_id is in the used_appid list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    app_id: int\n",
    "        App ID to check.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the app_id is in the used_appid list, False otherwise.\n",
    "    \"\"\"\n",
    "    return app_id in used_appid\n",
    "\n",
    "\n",
    "def fetch_steamspy_data(app_id_list, output_file):\n",
    "    \"\"\"\n",
    "    Perform get requests for a list of app IDs and write the output to a CSV file.\n",
    "\n",
    "    Parameters\n",
    "    --------\n",
    "    app_id_list: list of integers\n",
    "        List of app IDs to fetch data for.\n",
    "    output_file: str\n",
    "        Path to the output CSV file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    steamspy_games_data_json = []  # List to store the data\n",
    "\n",
    "    def parse(app_id):\n",
    "        url = f'https://steamspy.com/api.php?request=appdetails&appid={app_id}'\n",
    "        response = None\n",
    "        while response is None:\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 200:\n",
    "                    game_data = response.json()\n",
    "                    steamspy_games_data_json.append(game_data)\n",
    "                    used_appid.append(app_id)  # Add app_id to the used_appid list\n",
    "                else:\n",
    "                    print(f'Request for app_id {app_id} failed with status code: {response.status_code}')\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Request for app_id {app_id} failed with error: {e}\")\n",
    "            time.sleep(1)  # Delay for 1 second between requests\n",
    "\n",
    "            # Print progress after every 1000 requests\n",
    "            if len(used_appid) % 1000 == 0:\n",
    "                print('Processed {} requests on {}. Last app_id was {}'.format(len(used_appid), time.asctime(), app_id))\n",
    "\n",
    "    # Use a ThreadPoolExecutor to parallelize the requests\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        futures = [executor.submit(parse, app_id) for app_id in app_id_list]\n",
    "        for future in as_completed(futures):\n",
    "            future.result()\n",
    "\n",
    "    # Write the data to a CSV file\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=steamspy_columns)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(steamspy_games_data_json)\n",
    "\n",
    "    print(f\"Data written to {output_file} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 requests on Thu Jun 15 00:10:53 2023. Last app_id was 45700\n",
      "Processed 1000 requests on Thu Jun 15 00:10:53 2023. Last app_id was 45710\n",
      "Processed 1000 requests on Thu Jun 15 00:10:53 2023. Last app_id was 45730\n",
      "Processed 1000 requests on Thu Jun 15 00:10:53 2023. Last app_id was 45720\n",
      "Processed 2000 requests on Thu Jun 15 00:13:44 2023. Last app_id was 233700\n",
      "Processed 2000 requests on Thu Jun 15 00:13:44 2023. Last app_id was 233720\n",
      "Processed 3000 requests on Thu Jun 15 00:16:35 2023. Last app_id was 277910\n",
      "Processed 3000 requests on Thu Jun 15 00:16:35 2023. Last app_id was 277930\n",
      "Processed 4000 requests on Thu Jun 15 00:19:25 2023. Last app_id was 313870\n",
      "Processed 4000 requests on Thu Jun 15 00:19:25 2023. Last app_id was 313960\n",
      "Processed 6000 requests on Thu Jun 15 00:25:06 2023. Last app_id was 368180Processed 6000 requests on Thu Jun 15 00:25:06 2023. Last app_id was 368220\n",
      "\n",
      "Processed 6000 requests on Thu Jun 15 00:25:06 2023. Last app_id was 368230\n",
      "Processed 7000 requests on Thu Jun 15 00:27:56 2023. Last app_id was 394960\n",
      "Processed 7000 requests on Thu Jun 15 00:27:56 2023. Last app_id was 394970\n",
      "Processed 7000 requests on Thu Jun 15 00:27:56 2023. Last app_id was 394990\n",
      "Request for app_id 395470 failed with error: HTTPSConnectionPool(host='steamspy.com', port=443): Max retries exceeded with url: /api.php?request=appdetails&appid=395470 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x28e39c670>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Processed 12000 requests on Thu Jun 15 00:42:17 2023. Last app_id was 544440\n",
      "Processed 12000 requests on Thu Jun 15 00:42:17 2023. Last app_id was 544400\n",
      "Processed 12000 requests on Thu Jun 15 00:42:17 2023. Last app_id was 544480Processed 12000 requests on Thu Jun 15 00:42:17 2023. Last app_id was 544410\n",
      "\n",
      "Processed 14000 requests on Thu Jun 15 00:47:58 2023. Last app_id was 599070\n",
      "Processed 14000 requests on Thu Jun 15 00:47:58 2023. Last app_id was 599060Processed 14000 requests on Thu Jun 15 00:47:58 2023. Last app_id was 599140\n",
      "\n",
      "Processed 19000 requests on Thu Jun 15 01:02:21 2023. Last app_id was 739080\n",
      "Processed 19000 requests on Thu Jun 15 01:02:21 2023. Last app_id was 739100\n",
      "Processed 19000 requests on Thu Jun 15 01:02:21 2023. Last app_id was 739130\n",
      "Processed 19000 requests on Thu Jun 15 01:02:21 2023. Last app_id was 739170\n",
      "Processed 19000 requests on Thu Jun 15 01:02:21 2023. Last app_id was 739180\n",
      "Processed 19000 requests on Thu Jun 15 01:02:21 2023. Last app_id was 739190\n",
      "Processed 23000 requests on Thu Jun 15 01:14:13 2023. Last app_id was 845940\n",
      "Processed 23000 requests on Thu Jun 15 01:14:13 2023. Last app_id was 845960Processed 23000 requests on Thu Jun 15 01:14:13 2023. Last app_id was 845890Processed 23000 requests on Thu Jun 15 01:14:13 2023. Last app_id was 845900\n",
      "\n",
      "\n",
      "Processed 23000 requests on Thu Jun 15 01:14:13 2023. Last app_id was 845880\n",
      "Processed 53000 requests on Thu Jun 15 02:40:25 2023. Last app_id was 1868140Processed 53000 requests on Thu Jun 15 02:40:25 2023. Last app_id was 1868080\n",
      "\n",
      "Processed 53000 requests on Thu Jun 15 02:40:25 2023. Last app_id was 1868180Processed 53000 requests on Thu Jun 15 02:40:25 2023. Last app_id was 1868280Processed 53000 requests on Thu Jun 15 02:40:25 2023. Last app_id was 1868220\n",
      "\n",
      "\n",
      "Processed 53000 requests on Thu Jun 15 02:40:25 2023. Last app_id was 1868310\n",
      "Processed 53000 requests on Thu Jun 15 02:40:25 2023. Last app_id was 1868170Processed 53000 requests on Thu Jun 15 02:40:25 2023. Last app_id was 1868240\n",
      "\n",
      "Processed 54000 requests on Thu Jun 15 02:43:17 2023. Last app_id was 1918790\n",
      "Processed 54000 requests on Thu Jun 15 02:43:17 2023. Last app_id was 1918890Processed 54000 requests on Thu Jun 15 02:43:17 2023. Last app_id was 1918880\n",
      "\n",
      "Data written to steamspy_data_raw.csv successfully.\n"
     ]
    }
   ],
   "source": [
    "#getting data\n",
    "fetch_steamspy_data(list_of_appids,'steamspy_data_raw########.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LHL_Bootcamp",
   "language": "python",
   "name": "lhl_bootcamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
